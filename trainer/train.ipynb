{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching important feature based on 54 total features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pefile\n",
    "import sys\n",
    "import array\n",
    "import joblib\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "# Read data.csv file\n",
    "data = pd.read_csv('data.csv', sep='|')\n",
    "X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values\n",
    "y = data['legitimate'].values\n",
    "\n",
    "print('Searching important feature based on %i total features\\n' % X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most important features\n",
    "fsel = ske.ExtraTreesClassifier(verbose=1,  n_estimators=2000, criterion='entropy').fit(X, y)\n",
    "model = SelectFromModel(fsel, prefit=True, importance_getter=\"auto\")\n",
    "X_new = model.transform(X)\n",
    "nb_features = X_new.shape[1]\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.30, train_size=0.70, random_state=5,  shuffle=True)\n",
    "\n",
    "\n",
    "important_features = []\n",
    "\n",
    "print(f\"{nb_features} features identified as important:\")\n",
    "\n",
    "sorted_importances = np.argsort(fsel.feature_importances_)[::-1]\n",
    "top_indices = sorted_importances[:nb_features]\n",
    "for i, f in enumerate(top_indices):\n",
    "    feature_name = data.columns[2 + f]\n",
    "    feature_importance = fsel.feature_importances_[f]\n",
    "    print(f\"{i + 1}. feature {feature_name} ({feature_importance})\")\n",
    "\n",
    "sorted_top_indices = sorted(top_indices)\n",
    "for f in sorted_top_indices:\n",
    "    important_features.append(data.columns[2 + f])\n",
    "\n",
    "\n",
    "# Train and evaluate machine learning algorithms\n",
    "algorithms = {\n",
    "    \"DecisionTree\": tree.DecisionTreeClassifier(max_depth=10),\n",
    "    \"RandomForest1\": ske.RandomForestClassifier(n_estimators=150, max_features='log2'),\n",
    "    \"RandomForest2\": ske.RandomForestClassifier(n_estimators=1000, max_features='log2'),\n",
    "    \"RandomForest3\": ske.RandomForestClassifier(n_estimators=1500),\n",
    "    \"RandomForest4\": ske.RandomForestClassifier(n_estimators=2000),\n",
    "    \"GradientBoosting\": ske.GradientBoostingClassifier(n_estimators=50),\n",
    "    \"AdaBoost1\": ske.AdaBoostClassifier(n_estimators=100, learning_rate=1.5),\n",
    "    \"AdaBoost2\": ske.AdaBoostClassifier(n_estimators=300, learning_rate=1.5),\n",
    "    \"bdt_real\": ske.AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), n_estimators=350, learning_rate=1, algorithm=\"SAMME\"),\n",
    "    \"bdt_discrete\": ske.AdaBoostClassifier(DecisionTreeClassifier(max_depth=7), n_estimators=300, learning_rate=1.5, algorithm=\"SAMME\"),\n",
    "    \"GNB\": GaussianNB(),\n",
    "    \"KNN1\": KNeighborsClassifier(n_neighbors=3, algorithm='auto'),\n",
    "    \"KNN2\": KNeighborsClassifier(n_neighbors=2, algorithm='auto'),\n",
    "    \"XGBoost\": xgb.XGBClassifier()\n",
    "}\n",
    "results = {}\n",
    "for algo in algorithms:\n",
    "    clf = algorithms[algo]\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"%s : %f %%\" % (algo, score * 100))\n",
    "    results[algo] = score\n",
    "\n",
    "# Select the winning algorithm\n",
    "winner = max(results, key=results.get)\n",
    "print('\\nWinner algorithm is %s with a %f %% success' %\n",
    "      (winner, results[winner] * 100))\n",
    "\n",
    "# Save the winning algorithm and selected features\n",
    "joblib.dump(algorithms[winner], 'classifier/classifier.pkl')\n",
    "open('classifier/features.pkl', 'wb').write(pickle.dumps(important_features))\n",
    "\n",
    "# Calculate and print false positive and false negative rates\n",
    "clf = algorithms[winner]\n",
    "res = clf.predict(X_test)\n",
    "mt = confusion_matrix(y_test, res)\n",
    "print(\"False positive rate: %f %%\" % ((mt[0][1] / float(sum(mt[0]))) * 100))\n",
    "print(\"False negative rate: %f %%\" % ((mt[1][0] / float(sum(mt[1])) * 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "algorithm = {\n",
    "    \"SVM\": SVC(kernel='rbf', gamma='auto', verbose=True)\n",
    "}\n",
    "\n",
    "# Train and test the SVM algorithm\n",
    "classifier = algorithm[\"SVM\"]\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier_score = classifier.score(X_test, y_test)\n",
    "print(\"SVM : %f %%\" % (classifier_score * 100))\n",
    "result[\"SVM\"] = classifier_score\n",
    "\n",
    "# Save the results of the SVM algorithm in a pickle file\n",
    "joblib.dump(algorithm[\"SVM\"], 'classifier/svm_classifier.pkl')\n",
    "open('classifier/svm_features.pkl', 'wb').write(pickle.dumps(important_features))\n",
    "\n",
    "# Calculate and print false positive and false negative rates\n",
    "clfr = algorithm[\"SVM\"]\n",
    "reslt = clfr.predict(X_test)\n",
    "mt1 = confusion_matrix(y_test, reslt)\n",
    "print(\"False positive rate: %f %%\" % ((mt1[0][1] / float(sum(mt1[0]))) * 100))\n",
    "print(\"False negative rate: %f %%\" % ((mt1[1][0] / float(sum(mt1[1])) * 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the algorithm names and their scores\n",
    "algorithm_names = list(results.keys())\n",
    "scores = list(results.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,12))\n",
    "plt.plot(algorithm_names, scores, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance of different algorithms')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a2365cc9973f15c638ee603b3a7e6accec4ebdade2354b699a9686e8d003b90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
